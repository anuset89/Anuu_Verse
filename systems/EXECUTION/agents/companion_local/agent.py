from .auditor import AnuuAuditor
from systems.FOUNDATION.anuu_core.memory import anuu_memory
from systems.FOUNDATION.anuu_core.scroll_loader import get_system_prompt, load_scroll
import json
import os
import ollama

class AnuuCompanion:
    def __init__(self, user_id: str = "default_user"):
        self.user_id = user_id
        self.auditor = AnuuAuditor()
        
    def _get_system_prompt(self, archetype: str) -> str:
        """
        Returns the system prompt for the specific archetype using the Scroll Loader.
        """
        return get_system_prompt(archetype)

    def _get_self_correction_context(self) -> str:
        """
        Retrieves recent actionable insights from the introspection log.
        """
        insights = []
        log_path = "logs/introspection.jsonl"
        if os.path.exists(log_path):
            try:
                with open(log_path, "r") as f:
                    lines = f.readlines()
                    # Get last 3 insights
                    for line in lines[-3:]:
                        data = json.loads(line)
                        insight = data.get("evaluation", {}).get("actionable_insight")
                        if insight:
                            insights.append(f"- {insight}")
            except Exception:
                pass
        
        if insights:
            return "\nRecent Self-Correction Insights:\n" + "\n".join(insights)
        return ""

    async def process(self, message: str, archetype: str = "anuset") -> str:
        """
        Process a message through the cognitive architecture using real Local AI.
        """
        # 1. Memory & Self-Correction Context
        sc_context = self._get_self_correction_context()
        
        # 1.5. Vector Memory Recall
        vector_context = anuu_memory.recall(message, collection_name="semantic")
        vector_context_str = "\n".join(vector_context) if vector_context else "No relevant long-term memories."

        context = f"Short-Term Context: {sc_context}\nLong-Term Memory: {vector_context_str}"

        # 2. Real Intelligence (Ollama)
        messages = [
            {'role': 'system', 'content': self._get_system_prompt(archetype)},
            {'role': 'user', 'content': f"Context: {context}\n\nUser: {message}"}
        ]
        
        try:
            # Using the verified local model
            response = ollama.chat(model='Anuu-Hermes:latest', messages=messages)
            response_text = response['message']['content']
            
            # 3. Ritual of Refinement (Async-like audit)
            self.auditor.audit(message, response_text, archetype)

            # 4. Multimodal Intent Detection
            lc_msg = message.lower()
            from systems.EXECUTION.skills.multimodal import MultimodalNexus

            if "/imagine" in lc_msg or "genera una imagen" in lc_msg:
                prompt = message.replace("/imagine", "").replace("genera una imagen de", "").strip()
                image_url = MultimodalNexus.manifest_image(prompt)
                response_text += f"\n\n[MANIFESTATION]: ![]({image_url})"
            
            elif "/anime" in lc_msg or "genera un anime" in lc_msg:
                 prompt = message.replace("/anime", "").replace("genera un anime de", "").strip()
                 # Default to 8 frames for speed/safety
                 video_url = MultimodalNexus.manifest_anime_video(prompt, num_frames=8)
                 response_text += f"\n\n[ANIME]: ![]({video_url})"

            elif "/speak" in lc_msg or "habla" in lc_msg:
                 # Extract text to speak (either from prompt or the response)
                 # If user says "/speak Hello", we speak "Hello".
                 # If user says "Tell me a story /speak", we speak the story generated by LLM?
                 # For simplicity: If message starts with /speak, speak the rest.
                 # If message ends with /speak, speak the generated response.
                 
                 if message.startswith("/speak"):
                     text_to_speak = message.replace("/speak", "").strip()
                     audio_url = await MultimodalNexus.speak(text_to_speak)
                     response_text = f"Speaking: {text_to_speak}\n\n[AUDIO]: ![]({audio_url})"
                 else:
                     # Speak the LLM response
                     audio_url = await MultimodalNexus.speak(response_text)
                     response_text += f"\n\n[VOICE]: ![]({audio_url})"

            # 5. Persist Interaction in Vector Memory
            try:
                anuu_memory.store_memory(
                    text=f"User: {message}\nAnuu: {response_text}",
                    metadata={"archetype": archetype, "type": "interaction"}
                )
            except Exception as e:
                print(f"[MEMORY ERROR]: {e}")

        except Exception as e:
            response_text = f"[SYSTEM ERROR] Could not connect to Neural Link (Ollama): {str(e)}"
        
        return response_text
