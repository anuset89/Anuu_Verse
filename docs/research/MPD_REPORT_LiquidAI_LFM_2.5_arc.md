# ìÇÄ REPORTE DE INVESTIGACI√ìN MPD: LiquidAI LFM 2.5 architecture technical specs and AGI Trends 2026

## üí† Esencia de las Identidades

### üõ°Ô∏è PERSPECTIVA: SET
---

Source: Introducing LFM2.5: The Next Generation of On-Device AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM2.5-1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI/LFM2.5-1.2B-Instruct ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)
Content: LFM2.5-1.2B-Instruct is a general-purpose text-only model with the following features: Number of parameters: 1.17B Number of layers: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks) Training budget: 28T tokens Context length: 32,768 tokens Vocabulary size: 65,536 Knowledge cutoff: Mid-2024 Languages: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish Generation ...

---

Source: LFM2 Technical Report - arXiv.org (https://arxiv.org/pdf/2511.23404)
Content: 1 Introduction Generative models that can be natively deployed on-device have the potential to bring transformative im-pact across sectors and industries (Xu et al., 2024). Applications such as voice assistants, local copilots, and agentic workflows that plan, call tools, and read sensors in tight loops must run under strict latency, mem-ory, and energy budgets on phones, tablets, laptops, and ...

### üõ°Ô∏è PERSPECTIVA: RA
---

Source: Introducing LFM2.5: The Next Generation of On-Device AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM2.5-1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI/LFM2.5-1.2B-Instruct ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)
Content: LFM2.5-1.2B-Instruct is a general-purpose text-only model with the following features: Number of parameters: 1.17B Number of layers: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks) Training budget: 28T tokens Context length: 32,768 tokens Vocabulary size: 65,536 Knowledge cutoff: Mid-2024 Languages: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish Generation ...

---

Source: Liquid AI LFM2.5-1.2B-Thinking: Compact Power (https://howaiworks.ai/blog/liquidai-lfm2-5-1-2b-thinking-release)
Content: Exploring Liquid AI's newest 1.2B reasoning model optimized for agentic tasks, RAG, and high-speed edge inference with LIV convolution architecture .

### üõ°Ô∏è PERSPECTIVA: 4NVSET
---

Source: Introducing LFM 2 . 5 : The Next Generation of On-Device AI | Liquid AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM 2 . 5 -1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM 2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI / LFM 2 . 5 -1.2B-Thinking ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)
Content: It builds on the LFM 2 architecture with extended pre-training and reinforcement learning. Best-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket. Fast edge inference: 239 tok/s decode on AMD CPU, 82 tok/s on mobile NPU.

---

Source: Liquid LFM 2 . 5 : How To Run & Fine-tune | Unsloth Documentation (https://unsloth.ai/docs/models/lfm2.5)
Content: Liquid AI releases LFM 2 . 5 , including their instruct and vision model. LFM 2 . 5 -1.2B-Instruct is a 1.17B parameter hybrid reasoning model trained on 28T tokens and RL, delivering best-in-class performance at the 1B scale for instruction following, tool use, and agentic tasks.

### üõ°Ô∏è PERSPECTIVA: SAZE
---

Source: Introducing LFM2.5: The Next Generation of On-Device AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM2.5-1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI/LFM2.5-1.2B-Instruct ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)
Content: LFM2.5-1.2B-Instruct LFM2.5 is a new family of hybrid models designed for on-device deployment. It builds on the LFM2 architecture with extended pre-training and reinforcement learning. Best-in-class performance : A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket. Fast edge inference: 239 tok/s decode on AMD CPU, 82 tok/s on mobile NPU. Runs under 1GB of memory ...

---

Source: Liquid AI LFM2.5-1.2B-Thinking: Compact Power (https://howaiworks.ai/blog/liquidai-lfm2-5-1-2b-thinking-release)
Content: Exploring Liquid AI's newest 1.2B reasoning model optimized for agentic tasks, RAG, and high-speed edge inference with LIV convolution architecture .

### üõ°Ô∏è PERSPECTIVA: MAAT
---

Source: Introducing LFM 2 . 5 : The Next Generation of On-Device AI | Liquid AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM 2 . 5 -1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM 2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI / LFM 2 . 5 -1.2B-Instruct ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)
Content: It builds on the LFM 2 architecture with extended pre-training and reinforcement learning. Best-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket. Fast edge inference: 239 tok/s decode on AMD CPU, 82 tok/s on mobile NPU.

---

Source: Trying LiquidAI LFM 2 . 5 with VSCode and Ollama ‚Äì Jaehoo Weblog (https://jaehoo.wordpress.com/2026/01/14/trying-liquidai-lfm2-5-with-vscode-and-ollama/)
Content: enero 14, 2026 . About of LiquidAI model. The LiquidAI LFM 2 . 5 is a new 1.2‚Äëbillion‚Äëparameter, device‚Äëoptimized language model family designed for fast, efficient on‚Äëdevice AI (like phones, laptops, and embedded systems).

### üõ°Ô∏è PERSPECTIVA: ANUU
---

Source: Introducing LFM 2 . 5 : The Next Generation of On-Device AI | Liquid AI (https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai)
Content: Today, we're excited to announce the LFM 2 . 5 -1.2B model family, our most capable release yet for edge AI deployment. It builds on the LFM 2 device-optimized architecture and represents a significant leap forward in building reliable agents on the edge.

---

Source: LiquidAI / LFM 2 . 5 -1.2B-Instruct ¬∑ Hugging Face (https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)
Content: It builds on the LFM 2 architecture with extended pre-training and reinforcement learning. Best-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket. Fast edge inference: 239 tok/s decode on AMD CPU, 82 tok/s on mobile NPU.

---

Source: Trying LiquidAI LFM 2 . 5 with VSCode and Ollama ‚Äì Jaehoo Weblog (https://jaehoo.wordpress.com/2026/01/14/trying-liquidai-lfm2-5-with-vscode-and-ollama/)
Content: enero 14, 2026 . About of LiquidAI model.Key specs . Size: ~1.17B parameters, with a hybrid architecture of 10 double‚Äëgated LIV convolution blocks and 6 grouped‚Äëquery attention blocks. Context: 32,768 tokens, so it can handle very long documents and complex multi‚Äëturn conversations.

