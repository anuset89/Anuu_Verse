# üõ†Ô∏è Developer Guide: Anuu_Verse

**Target Audience:** Developers who want to extend, integrate, or contribute to Anuu_Verse.

---

## üèóÔ∏è Architecture Overview

Anuu_Verse is a **Multi-Agent Cognitive Framework** with three layers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRESENTATION LAYER                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ   Web UI     ‚îÇ   ‚îÇ  REST API    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ  (React)     ‚îÇ   ‚îÇ  (FastAPI)   ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ORCHESTRATION LAYER                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ     LangGraph Coordinator                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Routes tasks to appropriate identity          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Manages context & state                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     COGNITIVE LAYER                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇANUU ‚îÇ  ‚îÇKALI ‚îÇ  ‚îÇ SET ‚îÇ  ‚îÇKILO ‚îÇ  ‚îÇ ... ‚îÇ  (9 IDs)  ‚îÇ
‚îÇ  ‚îÇCore ‚îÇ  ‚îÇSec  ‚îÇ  ‚îÇAnal ‚îÇ  ‚îÇCrea ‚îÇ  ‚îÇ     ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PERSISTENCE LAYER                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  ChromaDB    ‚îÇ   ‚îÇ  Local LLM   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ (Vectors)    ‚îÇ   ‚îÇ  (Ollama)    ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÇ Directory Structure

```
Anuu_Verse/
‚îú‚îÄ‚îÄ systems/                    # Core cognitive systems
‚îÇ   ‚îú‚îÄ‚îÄ FOUNDATION/            # Base infrastructure
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anuu_core/         # Memory & core utilities
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ memory.py      # Vector DB interface
‚îÇ   ‚îú‚îÄ‚îÄ PERCEPTION/            # Input processing
‚îÇ   ‚îú‚îÄ‚îÄ COGNITION/             # Decision making
‚îÇ   ‚îú‚îÄ‚îÄ EXECUTION/             # Output generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ companion_local/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ main.py    # FastAPI server
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ agent.py   # AnuuCompanion class
‚îÇ   ‚îî‚îÄ‚îÄ AESTHETICS/            # Style & presentation
‚îÇ
‚îú‚îÄ‚îÄ web/                       # React frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/             # Main views
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/        # Reusable UI
‚îÇ   ‚îî‚îÄ‚îÄ public/                # Static assets
‚îÇ
‚îú‚îÄ‚îÄ examples/                  # Executable demos
‚îÇ   ‚îú‚îÄ‚îÄ basic_task.py
‚îÇ   ‚îî‚îÄ‚îÄ agent_selection.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ INDEX.md               # Main entry point
‚îÇ   ‚îú‚îÄ‚îÄ JOURNEY.md             # Development log
‚îÇ   ‚îî‚îÄ‚îÄ identities/            # Identity specs
‚îÇ
‚îî‚îÄ‚îÄ Library/                   # Shared utilities
```

---

## üß© Core Components

### 1. Memory System (`systems/FOUNDATION/anuu_core/memory.py`)

**Purpose:** Persistent context storage using vector embeddings.

**Interface:**
```python
class MemoryCore:
    def store_memory(text: str, metadata: dict) -> str:
        """Store a memory fragment. Returns memory ID."""
        
    def recall(query: str, n_results: int = 3) -> List[str]:
        """Retrieve relevant memories via semantic search."""
```

**Usage:**
```python
from systems.FOUNDATION.anuu_core.memory import anuu_memory

# Store
mem_id = anuu_memory.store_memory(
    "User prefers dark mode",
    metadata={"category": "preferences"}
)

# Recall
results = anuu_memory.recall("What does user like?")
```

### 2. Agent System (`systems/EXECUTION/agents/companion_local/agent.py`)

**Purpose:** Identity-based task processing.

**Interface:**
```python
class AnuuCompanion:
    def process(message: str, archetype: str = "anuset") -> str:
        """Process message through specified identity."""
```

**Current Identities:**
- `anuset` (default) ‚Äî Balanced, general-purpose
- `kali` ‚Äî Security/pentesting focus
- `set` ‚Äî Deep analysis
- `kilonova` ‚Äî Creative generation

### 3. API Server (`systems/EXECUTION/agents/companion_local/main.py`)

**Endpoints:**
```
GET  /              ‚Üí Health check
POST /chat          ‚Üí Process message
```

**Example Request:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analyze this security issue",
    "archetype": "kali",
    "user_id": "user_123"
  }'
```

---

## üîß How To: Add a New Identity

### Step 1: Create Identity Configuration

Create `systems/EXECUTION/agents/[identity_name]/config.py`:

```python
IDENTITY_CONFIG = {
    "name": "researcher",
    "system_prompt": """
    You are a meticulous research assistant.
    Your role: synthesize information from multiple sources.
    """,
    "tools": ["web_search", "summarize"],
    "temperature": 0.3,  # Lower for precision
}
```

### Step 2: Register in Agent Class

Update `agent.py`:

```python
IDENTITY_REGISTRY = {
    "anuset": {...},
    "kali": {...},
    "researcher": RESEARCHER_CONFIG,  # Add yours
}
```

### Step 3: Document in `docs/identities/`

Create `docs/identities/RESEARCHER.md`:

```markdown
# Researcher Identity

**Domain:** Information synthesis
**Specialty:** Multi-source analysis
**Use Case:** Academic research, fact-checking
```

### Step 4: Add Example

Create `examples/researcher_demo.py`:

```python
from systems.EXECUTION.agents.companion_local import AnuuCompanion

anuu = AnuuCompanion()
result = anuu.process(
    "Summarize recent papers on multi-agent systems",
    archetype="researcher"
)
print(result)
```

---

## üîå How To: Integrate a New LLM Backend

Currently using Ollama (mocked). To add a real integration:

### Step 1: Create Backend Adapter

Create `systems/FOUNDATION/anuu_core/llm_backends/ollama.py`:

```python
import ollama

class OllamaBackend:
    def __init__(self, model: str = "llama3"):
        self.model = model
    
    def generate(self, prompt: str) -> str:
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response['message']['content']
```

### Step 2: Update Agent to Use Backend

In `agent.py`:

```python
from systems.FOUNDATION.anuu_core.llm_backends.ollama import OllamaBackend

class AnuuCompanion:
    def __init__(self):
        self.llm = OllamaBackend(model="llama3")
    
    def process(self, message: str, archetype: str) -> str:
        prompt = f"[{archetype}]: {message}"
        return self.llm.generate(prompt)
```

### Step 3: Test

```bash
# Make sure Ollama is running
ollama serve

# Run example
python examples/basic_task.py
```

---

## üê≥ Development Environment

### Option 1: Local Python

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python systems/EXECUTION/agents/companion_local/main.py
```

### Option 2: Docker (Recommended)

```bash
docker-compose up
# API: http://localhost:8000
# Web: http://localhost:3000
```

---

## üß™ Testing

### Run Unit Tests

```bash
pytest tests/
```

### Test a Specific Module

```bash
pytest tests/test_memory.py -v
```

### Integration Test (Full System)

```bash
python tests/integration/test_full_flow.py
```

---

## üìä Performance Considerations

### Memory Usage
- ChromaDB keeps vectors in memory ‚Üí RAM scales with history
- Consider periodic cleanup or archiving for long-running instances

### LLM Latency
- Local models: 1-5s response time (depends on GPU)
- Cloud fallback: Add timeout parameters

### Concurrency
- FastAPI handles concurrent requests
- LangGraph manages agent coordination
- Memory writes are sequential (ChromaDB limitation)

---

## ü§ù Contributing Workflow

1. **Fork & Clone**
   ```bash
   git clone https://github.com/YOUR_USERNAME/Anuu_Verse.git
   ```

2. **Create Feature Branch**
   ```bash
   git checkout -b feature/my-new-identity
   ```

3. **Make Changes**
   - Add code
   - Update docs
   - Add tests

4. **Test Locally**
   ```bash
   pytest tests/
   python examples/basic_task.py
   ```

5. **Submit PR**
   - Clear description
   - Link to issue if applicable
   - Request review

---

## üêõ Debugging Tips

### Issue: Memory not persisting
**Solution:** Check `memory_db/` directory exists and has write permissions

### Issue: LLM not responding
**Solution:** 
```bash
# Check Ollama is running
curl http://localhost:11434/api/version

# Check model is available
ollama list
```

### Issue: Web UI not loading
**Solution:**
```bash
cd web
npm install
npm run dev
```

---

## üìö Further Reading

- [Architecture Deep Dive](./ARCHITECTURE.md)
- [Philosophy](./PHILOSOPHY.md)
- [Identity System](./identities/README.md)
- [API Reference](https://anuset89.github.io/Anuu_Verse/)

---

**Questions?** Open a [Discussion](https://github.com/anuset89/Anuu_Verse/discussions) or check [CONTRIBUTING.md](../CONTRIBUTING.md).

üü£
