# Arquitectura de Flujo Cognitivo de Anuu (ACE-01)

Este diagrama representa el flujo de informaciÃ³n desde la entrada del usuario hasta la manifestaciÃ³n final en el Dashboard.

```mermaid
graph TD
    %% Nodos Principales
    User([ğŸ‘¤ User Input])
    Router{ğŸ”€ Intent Router<br>(The Cortex)}
    Memory[(ğŸ§  Memory & Context)]
    
    %% Rutas de Procesamiento
    subgraph Cognitive_Core [The Ace Graph]
        LLM[ğŸ¤– Language Model<br>(Ollama / ACE)]
        Auditor[ğŸ›¡ï¸ Auditor<br>(Self-Correction)]
    end
    
    subgraph Creative_Nexus [Multimodal Nexus]
        ImgGen[ğŸ¨ Image Generator<br>(ComfyUI: PonyXL)]
        VidGen[ğŸ¬ Video Generator<br>(ComfyUI: AnimateDiff)]
        AudioGen[ğŸ—£ï¸ Audio Synth<br>(Edge-TTS)]
    end
    
    %% Flujo 
    User --> Memory
    Memory --> LLM
    LLM --> Router
    
    %% Routing Logic based on Agent.py
    Router -- "Text Response" --> Output([ğŸ“ Final Output])
    Router -- "/imagine" --> ImgGen
    Router -- "/anime" --> VidGen
    Router -- "/speak" --> AudioGen
    
    %% Aggregation
    ImgGen --> Output
    VidGen --> Output
    AudioGen --> Output
    
    %% Feedback Loop
    Output -.-> Memory
```

## DescripciÃ³n de Componentes

1.  **Chat (Input)**: El usuario envÃ­a un mensaje o comando desde el Nexus Dashboard.
2.  **LM (Cognition)**: El **ACE Graph** procesa el contexto y genera la respuesta textual base.
3.  **Router (Cortex)**: Analiza tanto el input original como la respuesta del LM para detectar intenciones creativas (`/imagine`, `/anime`, `/speak`) y referencias implÃ­citas ("GenÃ©rala").
4.  **Nexus (Manifestation)**:
    *   **ImÃ¡genes**: Llamada a la API de ComfyUI (Workflow `txt2img_pony`).
    *   **Video**: Llamada a la API de ComfyUI (Workflow `txt2vid_animatediff`).
    *   **Audio**: SÃ­ntesis de voz directa.
5.  **Output**: Se combina el texto y los enlaces a los medios generados (`/generations/...`) para su visualizaciÃ³n en el Dashboard.
