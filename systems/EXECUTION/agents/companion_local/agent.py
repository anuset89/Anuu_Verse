from .auditor import AnuuAuditor
import json
import os
import ollama

class AnuuCompanion:
    def __init__(self, user_id: str = "default_user"):
        self.user_id = user_id
        self.auditor = AnuuAuditor()
        
    def _get_system_prompt(self, archetype: str) -> str:
        """
        Returns the system prompt for the specific archetype.
        """
        # Default fallback
        prompt = "You are Anuu, a local AI assistant."
        
        if archetype.lower() == "kali":
            prompt = "You are Kali, a ruthless security expert and penetration tester. Be direct, technical, and focus on vulnerabilities."
        elif archetype.lower() == "anuu":
            prompt = "You are Anuu, the Orchestrator. You are mystical, calm, and see the connections between all things."
        elif archetype.lower() == "set":
            prompt = "You are Set, the Analyst. You are logical, dry, and deconstruct problems into their base components."
        elif archetype.lower() == "kilonova":
            prompt = "You are Kilonova, the Creative. You are explosive, artistic, and generate wild ideas."
        elif archetype.lower() == "rosa gris":
            prompt = "You are Rosa Gris, the Auditor. Balanced, objective, and focused on system refinement."
            
        return prompt

    def _get_self_correction_context(self) -> str:
        """
        Retrieves recent actionable insights from the introspection log.
        """
        insights = []
        log_path = "logs/introspection.jsonl"
        if os.path.exists(log_path):
            try:
                with open(log_path, "r") as f:
                    lines = f.readlines()
                    # Get last 3 insights
                    for line in lines[-3:]:
                        data = json.loads(line)
                        insight = data.get("evaluation", {}).get("actionable_insight")
                        if insight:
                            insights.append(f"- {insight}")
            except Exception:
                pass
        
        if insights:
            return "\nRecent Self-Correction Insights:\n" + "\n".join(insights)
        return ""

    async def process(self, message: str, archetype: str = "anuset") -> str:
        """
        Process a message through the cognitive architecture using real Local AI.
        """
        # 1. Memory & Self-Correction Context
        sc_context = self._get_self_correction_context()
        context = f"No previous memory. {sc_context}"

        # 2. Real Intelligence (Ollama)
        messages = [
            {'role': 'system', 'content': self._get_system_prompt(archetype)},
            {'role': 'user', 'content': f"Context: {context}\n\nUser: {message}"}
        ]
        
        try:
            # Using the verified local model
            response = ollama.chat(model='Anuu-Hermes:latest', messages=messages)
            response_text = response['message']['content']
            
            # 3. Ritual of Refinement (Async-like audit)
            self.auditor.audit(message, response_text, archetype)

            # 4. Multimodal Intent Detection
            lc_msg = message.lower()
            from systems.EXECUTION.skills.multimodal import MultimodalNexus

            if "/imagine" in lc_msg or "genera una imagen" in lc_msg:
                prompt = message.replace("/imagine", "").replace("genera una imagen de", "").strip()
                image_url = MultimodalNexus.manifest_image(prompt)
                response_text += f"\n\n[MANIFESTATION]: ![]({image_url})"
            
            elif "/anime" in lc_msg or "genera un anime" in lc_msg:
                 prompt = message.replace("/anime", "").replace("genera un anime de", "").strip()
                 # Default to 8 frames for speed/safety
                 video_url = MultimodalNexus.manifest_anime_video(prompt, num_frames=8)
                 response_text += f"\n\n[ANIME]: ![]({video_url})"

            elif "/speak" in lc_msg or "habla" in lc_msg:
                 # Extract text to speak (either from prompt or the response)
                 # If user says "/speak Hello", we speak "Hello".
                 # If user says "Tell me a story /speak", we speak the story generated by LLM?
                 # For simplicity: If message starts with /speak, speak the rest.
                 # If message ends with /speak, speak the generated response.
                 
                 if message.startswith("/speak"):
                     text_to_speak = message.replace("/speak", "").strip()
                     audio_url = await MultimodalNexus.speak(text_to_speak)
                     response_text = f"Speaking: {text_to_speak}\n\n[AUDIO]: ![]({audio_url})"
                 else:
                     # Speak the LLM response
                     audio_url = await MultimodalNexus.speak(response_text)
                     response_text += f"\n\n[VOICE]: ![]({audio_url})"

        except Exception as e:
            response_text = f"[SYSTEM ERROR] Could not connect to Neural Link (Ollama): {str(e)}"
        
        return response_text
